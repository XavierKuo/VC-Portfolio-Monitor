# ==========================================================
# main.py (Ver 5.7 - å®Œæ•´æ•´åˆç‰ˆ)
# é‚è¼¯æ›´æ–°: æ”¯æ´è‹±æ–‡æ¨™ç±¤ã€gpt-4o-miniã€å„ªåŒ–ç„¡æ¶ˆæ¯åˆ¤æ–·
# ç’°å¢ƒé©é…: GitHub Actions Secrets
# ==========================================================
import requests
import json
import gspread
import time
import sys
import os
from oauth2client.service_account import ServiceAccountCredentials
from datetime import datetime, timedelta

# ===========================
# 1. ç’°å¢ƒè®Šæ•¸èˆ‡å…¨åŸŸè¨­å®š (ä¿ç•™ GitHub Actions è¨­å®š)
# ===========================

# å¾ GitHub Secrets è®€å– Keys (å¦‚æœè®€ä¸åˆ°å‰‡ç‚º None)
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
SERPER_API_KEY = os.environ.get("SERPER_API_KEY")
TELEGRAM_BOT_TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.environ.get("TELEGRAM_CHAT_ID")

# æª”æ¡ˆåç¨± (å¿…é ˆèˆ‡ Google Sheet ä¸€è‡´)
SHEET_NAME = "VC_Portfolio_Config"

# è™•ç† Google JSON
# GitHub Secret å­˜çš„æ˜¯å­—ä¸²ï¼Œé€™è£¡å¿…é ˆè½‰å› Python å­—å…¸
google_json_str = os.environ.get("GOOGLE_JSON", "{}")
try:
    GOOGLE_CREDS_JSON = json.loads(google_json_str)
except json.JSONDecodeError:
    print("âŒ éŒ¯èª¤: GOOGLE_JSON æ ¼å¼ä¸æ­£ç¢ºï¼Œè«‹ç¢ºä¿è²¼ä¸Šçš„æ˜¯ç´” JSON å…§å®¹ã€‚")
    GOOGLE_CREDS_JSON = {}

# å…¨åŸŸè®Šæ•¸å®¹å™¨
PORTFOLIO_CONFIG = {}
MEDIA_SOURCES = {}
GLOBAL_SOCIAL_SITES = []
REGIONS = {
    "TW": {"hl": "zh-TW", "gl": "tw", "name": "å°ç£"},
    "JP": {"hl": "ja", "gl": "jp", "name": "æ—¥æœ¬"},
    "US": {"hl": "en", "gl": "us", "name": "ç¾åœ‹"},
}

# ===========================
# 2. æ ¸å¿ƒåŠŸèƒ½å‡½å¼å€ (æ›´æ–°è‡³ Ver 5.5 Custom)
# ===========================

# --- 1. Telegram ç™¼é€å‡½å¼ ---
def send_telegram_message(message):
    """ç™¼é€ Telegram è¨Šæ¯"""
    if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
        print("âš ï¸ Telegram Token æœªè¨­å®šï¼Œè·³éç™¼é€ã€‚")
        return

    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    # è¨Šæ¯åˆ†æ®µè™•ç† (Telegram é™åˆ¶ 4096 å­—å…ƒ)
    max_length = 4000
    parts = [message[i:i+max_length] for i in range(0, len(message), max_length)]
    
    for part in parts:
        payload = {
            "chat_id": TELEGRAM_CHAT_ID,
            "text": part,
            "parse_mode": "Markdown",
            "disable_web_page_preview": True
        }
        try:
            requests.post(url, json=payload)
            time.sleep(1)
        except Exception as e:
            print(f"âŒ Telegram ç™¼é€å¤±æ•—: {e}")

# --- 2. Google Sheet è®€å–å‡½å¼ ---
def load_all_config_from_sheets():
    """å¾ Google Sheet è®€å–æ‰€æœ‰é…ç½®"""
    global PORTFOLIO_CONFIG, MEDIA_SOURCES, GLOBAL_SOCIAL_SITES
    
    try:
        scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']
        
        if not GOOGLE_CREDS_JSON:
            error_msg = "âŒ éŒ¯èª¤: GOOGLE_CREDS_JSON æ˜¯ç©ºçš„ï¼Œç„¡æ³•é€£ç·šã€‚"
            print(error_msg)
            return False

        creds = ServiceAccountCredentials.from_json_keyfile_dict(GOOGLE_CREDS_JSON, scope)
        
        print(f"â„¹ï¸ æ­£åœ¨å˜—è©¦é€£ç·š Google Sheet...")
        print(f"â„¹ï¸ æ‚¨çš„ Service Account Email æ˜¯: ã€ {creds.service_account_email} ã€‘")
        
        client = gspread.authorize(creds)
        spreadsheet = client.open(SHEET_NAME)
        
        # 1. è®€å– Portfolio
        portfolio_sheet = spreadsheet.worksheet("Portfolio")
        portfolio_records = portfolio_sheet.get_all_records()
        
        PORTFOLIO_CONFIG = {}
        for row in portfolio_records:
            company = row.get('Company')
            if not company: continue
            
            regions_str = row.get('Regions', 'TW')
            # éæ¿¾æœ‰æ•ˆåœ°å€
            regions = [r.strip() for r in regions_str.split(',') if r.strip() in REGIONS]
            
            keywords_str = row.get('Keywords', company)
            keywords = [k.strip() for k in keywords_str.split(',') if k.strip()]
            
            PORTFOLIO_CONFIG[company] = {"regions": regions, "keywords": keywords}
        print(f"âœ… æˆåŠŸè¼‰å…¥ {len(PORTFOLIO_CONFIG)} é–“ Portfolio è¨­å®šã€‚")

        # 2. è®€å– Media Sources
        try:
            media_sheet = spreadsheet.worksheet("Media_Sources")
            media_records = media_sheet.get_all_records()
            MEDIA_SOURCES = {}
            for row in media_records:
                code = row.get('Region', '').upper()
                sites_str = row.get('Sites', '')
                sites = [s.strip() for s in sites_str.split(',') if s.strip()]
                if code and sites:
                    MEDIA_SOURCES[code] = sites
            print(f"âœ… æˆåŠŸè¼‰å…¥ {len(MEDIA_SOURCES)} å€‹åœ°å€åª’é«”é…ç½®ã€‚")
        except gspread.WorksheetNotFound:
             print("âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ° 'Media_Sources' åˆ†é ï¼Œå°‡ä½¿ç”¨é è¨­å€¼ã€‚")

        # 3. è®€å– Global Settings
        try:
            global_sheet = spreadsheet.worksheet("Global_Settings")
            global_records = global_sheet.get_all_records()
            global_settings = {}
            for row in global_records:
                global_settings[row.get('Setting_Name')] = row.get('Value')
            
            social_sites_str = global_settings.get('GLOBAL_SOCIAL_SITES', 'site:linkedin.com')
            GLOBAL_SOCIAL_SITES = [s.strip() for s in social_sites_str.split(',') if s.strip()]
            print(f"âœ… æˆåŠŸè¼‰å…¥å…¨åŸŸç¤¾ç¾¤ç®¡é“ã€‚")
        except gspread.WorksheetNotFound:
             print("âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ° 'Global_Settings' åˆ†é ï¼Œå°‡ä½¿ç”¨é è¨­å€¼ã€‚")
             GLOBAL_SOCIAL_SITES = ["site:linkedin.com"]
        
        return True
    
    except Exception as e:
        print(f"âŒ Google Sheet è®€å–ç™¼ç”ŸéŒ¯èª¤: {e}")
        return False

# --- 3. æœå°‹å‡½å¼ ---
def search_google_news(query, hl="zh-TW", gl="tw"):
    """
    ä½¿ç”¨ Serper API æœå°‹ Google News
    [é‡è¦é™„è¨»]ï¼š
    æœ¬å‡½å¼æ”¯æ´å…¨çƒæœå°‹ï¼Œé€éåƒæ•¸æ§åˆ¶ï¼š
    - hl (Host Language): æ§åˆ¶ä»‹é¢èªè¨€ (å¦‚ 'zh-TW', 'ja', 'en')
    - gl (Geo Location): æ§åˆ¶æœå°‹åœ°å€ (å¦‚ 'tw', 'jp', 'us')
    é€™äº›åƒæ•¸æ˜¯ç”± Google Sheet è¨­å®šæª”ä¸­çš„ 'Regions' æ¬„ä½å‹•æ…‹å‚³å…¥çš„ï¼Œ
    å› æ­¤å¯ä»¥å®Œç¾æ”¯æ´æ—¥æœ¬ (JP) èˆ‡ç¾åœ‹ (US) çš„åœ¨åœ°åŒ–æœå°‹ã€‚
    """
    url = "https://google.serper.dev/search"
    payload = json.dumps({
        "q": query,
        "tbs": "qdr:w", # é™åˆ¶éå»ä¸€é€±
        "num": 20,      # å¢åŠ æœå°‹æ•¸é‡ä»¥æé«˜å‘½ä¸­ç‡
        "hl": hl,
        "gl": gl
    })
    headers = {'X-API-KEY': SERPER_API_KEY, 'Content-Type': 'application/json'}
    try:
        response = requests.request("POST", url, headers=headers, data=payload)
        return response.json()
    except Exception as e:
        return {"error": str(e)}

# --- 4. AI åˆ†æå‡½å¼ ---
def analyze_with_gpt(company_name, all_search_results_list):
    # [è¨­å®š] OpenAI æ¨¡å‹é¸æ“‡
    OPENAI_MODEL_NAME = "gpt-4o" 

    all_organic_results = []
    seen_links = set()
    
    # è³‡æ–™æ¸…æ´—èˆ‡å»é‡
    for result_dict in all_search_results_list:
        if 'organic' in result_dict:
            for item in result_dict['organic']:
                link = item.get('link')
                if link and link not in seen_links:
                    all_organic_results.append(item)
                    seen_links.add(link)
    
    if not all_organic_results: return None

    # æ™‚é–“è¨­å®š
    today = datetime.now()
    seven_days_ago = today - timedelta(days=7)
    today_str = today.strftime("%Y-%m-%d")
    seven_days_ago_str = seven_days_ago.strftime("%Y-%m-%d")

    # å»ºæ§‹ Context
    news_text = ""
    for item in all_organic_results[:20]:
        title = item.get('title', 'No Title')
        snippet = item.get('snippet', 'No Snippet')
        link = item.get('link', '')
        date = item.get('date', 'Unknown Date')
        news_text += f"- [Date: {date}] {title} ({link}): {snippet}\n"

    # [è¨­å®š] å„ªåŒ–å¾Œçš„ System Prompt (å…¨ç¹é«”ä¸­æ–‡è¼¸å‡ºç‰ˆ)
    prompt = f"""
    You are a strict VC investment analyst. Today is: {today_str}.
    Task: Review the global search results for portfolio company "{company_name}".

    ã€Time Filterã€‘
    - Focus on news between **{seven_days_ago_str} and {today_str}**.
    - **Important Exception**: If a news item has NO date or an ambiguous date (e.g., "Recent"), but the content seems highly relevant and new, **INCLUDE IT**. Do not miss major events due to missing date tags.
    - Only exclude news clearly marked as "1 year ago", "2023", etc.
    - If no relevant news at all, reply exactly: "No huge updates".

    ã€Consolidation & Deduplicationã€‘
    - **CRITICAL**: If multiple sources report the same event, consolidate them into ONE summary.
    
    ã€High Value Signals & Categorizationã€‘
    Classify news into these **English Tags** ONLY:
    1. [ğŸš¨ CRISIS] (PR crisis, viral controversy, lawsuits)
    2. [ğŸ’° FUNDING] (Fundraising, M&A, IPO)
    3. [ğŸš€ PRODUCT] (New product launch, New markets expansion)
    4. [ğŸ“¢ EVENT] (Major brand events, exhibitions)
    5. [ğŸ¤ PARTNERSHIP] (Strategic alliances)
    6. [ğŸ‘¤ PEOPLE] (C-Level changes)

    ã€Output Language Rulesã€‘
    - **Global Translation**: Regardless of the source language (English, Japanese, etc.), ALL outputs (Titles and Summaries) must be in **Traditional Chinese (ç¹é«”ä¸­æ–‡)**.
    - **Tag Retention**: Keep the Categorization Tags in **English** (e.g., [ğŸ’° FUNDING]).
    - Summary Length: Concise, approximately **50-100 characters**.

    ã€Databaseã€‘
    {news_text}

    ã€Output Formatã€‘
    If news exists, output in this exact format:

    **Tag | Title (in Traditional Chinese)**
    - (Summary in Traditional Chinese)
    ğŸ” Source: [Link Title](Link) (Provide only 1 best source link)
    """

    url = "https://api.openai.com/v1/chat/completions"
    headers = {"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"}
    data = {"model": OPENAI_MODEL_NAME, "messages": [{"role": "user", "content": prompt}], "temperature": 0.0}
    
    try:
        response = requests.post(url, headers=headers, json=data)
        result = response.json()
        if 'error' in result: return f"API Error: {result['error']['message']}"
        content = result['choices'][0]['message']['content']
        
        # åµæ¸¬ç„¡æ¶ˆæ¯çš„é—œéµå­—
        if "No huge updates" in content or "ç„¡é‡å¤§æ¶ˆæ¯" in content:
            return None
            
        return content
    except Exception as e:
        return f"ç¨‹å¼åŸ·è¡ŒéŒ¯èª¤: {str(e)}"

# ===========================
# 3. ä¸»ç¨‹å¼åŸ·è¡Œé‚è¼¯å€ (Ver 5.3 å®Œæ•´ä¿®å¾©ç‰ˆ)
# ===========================
if __name__ == "__main__":
    print("ğŸš€ é–‹å§‹åŸ·è¡Œ VC Portfolio Tracker (GitHub Actions Mode)...\n")

    # 1. è¼‰å…¥é…ç½® (åŒ…å«éŒ¯èª¤æª¢æŸ¥)
    if not load_all_config_from_sheets():
        error_msg = f"âŒ åš´é‡éŒ¯èª¤: ç„¡æ³•å¾ Google Sheet è¼‰å…¥é…ç½®ã€‚æª¢æŸ¥ GitHub Secrets å’Œ Sheet å…±ç”¨æ¬Šé™ã€‚"
        print(error_msg)
        send_telegram_message(error_msg)
        sys.exit(1) # çµ‚æ­¢ç¨‹å¼

    final_report_sections = []
    stats = {
        "total_tracked": len(PORTFOLIO_CONFIG),
        "news_found": 0,
        "regions_scanned": set(),
        "time_start": datetime.now(),
    }
    successful_scans = 0 # æˆåŠŸæœå°‹æ¬¡æ•¸è¨ˆæ•¸å™¨

    # 2. åŸ·è¡Œæƒæ
    for company_name, config in PORTFOLIO_CONFIG.items():
        keywords = config["keywords"]
        target_regions = config["regions"]

        print(f"\n--- åˆ†æ: {company_name} ---")

        all_search_results = []
        # çµ„åˆå…¨åŸŸç¤¾ç¾¤é—œéµå­—
        all_search_terms = keywords + GLOBAL_SOCIAL_SITES

        for region_code in target_regions:
            if region_code not in REGIONS: continue

            # çµ±è¨ˆæƒæåœ°å€
            stats["regions_scanned"].add(region_code)

            region_info = REGIONS[region_code]
            # å–å¾—åœ°å€åª’é«”è¨­å®š
            regional_media = MEDIA_SOURCES.get(region_code, [])

            # çµ„åˆæŸ¥è©¢
            combined_query = " OR ".join(all_search_terms + regional_media)

            # åŸ·è¡Œæœå°‹
            search_res = search_google_news(combined_query, hl=region_info["hl"], gl=region_info["gl"])

            if "error" in search_res:
                print(f"   âŒ {region_info['name']} æœå°‹éŒ¯èª¤: {search_res['error']}")
            else:
                all_search_results.append(search_res)
                successful_scans += 1

        # AI åˆ†æ (å¦‚æœæœ‰æœå°‹çµæœ)
        if all_search_results:
            print("   ğŸ¤– æ­£åœ¨é€²è¡Œ AI ç¶œåˆåˆ†æ...")
            analysis = analyze_with_gpt(company_name, all_search_results)

            if analysis and "No huge updates" not in analysis and "API Error" not in analysis:
                print(f"   âœ… {company_name} Something happened!")
                stats["news_found"] += 1
                final_report_sections.append(f"*{company_name}*\n{analysis}\n")
            else:
                print(f"   ğŸ’¤ {company_name} No huge updates~")

        time.sleep(1) # é¿å… API é€Ÿç‡é™åˆ¶

    # 3. ç”Ÿæˆå ±å‘Š
    time_taken = datetime.now() - stats["time_start"]

    # è¨ˆç®—æˆåŠŸç‡
    total_expected = stats['total_tracked'] * len(stats['regions_scanned']) if stats['regions_scanned'] else 1
    success_rate = f"{successful_scans / total_expected * 100:.0f}%" if total_expected > 0 else "0%"

    header = f"ğŸ¤– *Daily Portfolio Monitor*\nğŸ“… Date: {datetime.now().strftime('%Y-%m-%d')}\n"
    stats_block = (
        "\nğŸ“Š *Summary Statistics:*\n"
        f"â€¢ Companies: {stats['total_tracked']}\n"
        f"â€¢ Updates: {stats['news_found']}\n"
        f"â€¢ Success Rate: {success_rate}\n"
        f"â€¢ Time: {str(time_taken).split('.')[0]}\n\n"
        "ğŸ“ *Highlights:*\n" + "-"*15 + "\n"
    )

    if final_report_sections:
        body = "\n".join(final_report_sections)
        full_report = header + stats_block + body
    else:
        full_report = header + stats_block + "æœ¬é€± Portfolio å¹³éœç„¡æ³¢ã€‚"

    # 4. ç™¼é€ Telegram
    print("\næ­£åœ¨ç™¼é€ Telegram å ±å‘Š...")
    send_telegram_message(full_report)
    print("âœ… å®Œæˆï¼")
